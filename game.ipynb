{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **This game serves as a simple illustration of a reinforcement learning (RL) environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The Game Setup**\n",
    " **Grid Environment:** The game is set in a 5x5 grid, where each cell can represent either a free space, an obstacle, the agent (Rushi), or the target (Sleep).\n",
    "\n",
    " **Agent (Rushi):** The agent starts at the top-left corner of the grid and needs to navigate to the target.\n",
    "\n",
    " **Obstacles** (Instagram, Movies, YouTube): Placed in specific locations within the grid, hitting an obstacle results in a penalty, and Rushi is reset to the starting position.\n",
    "\n",
    " **Target (Sleep):** The goal for Rushi is to reach the target located at the bottom-right corner of the grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reinforcement Learning Concepts**\n",
    "**States:** Each position of Rushi in the grid represents a unique state of the environment.\n",
    "\n",
    "**Actions:** In each state, Rushi can perform one of four actions: move up, down, left, or right.\n",
    "\n",
    "**Rewards:** The game provides feedback through rewards:\n",
    "\n",
    "    1. Rushi receives a positive reward for reaching Sleep.\n",
    "    \n",
    "    2. A negative reward (penalty) is given each time Rushi hits an obstacle or takes a step, encouraging efficiency.\n",
    "    \n",
    "**Q-table:** The game uses a Q-learning algorithm, where a Q-table records the expected future rewards for each action in every state. Rushi uses this table to decide which action to take next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 1\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "class GridGameGUI:\n",
    "    def __init__(self, size=5, epochs=1000, target_threshold=1):\n",
    "        self.size = size\n",
    "        self.epochs = epochs\n",
    "        self.target_reached_count = 0\n",
    "        self.target_threshold = target_threshold\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.obstacles = {'Instagram': [1, 1], 'Movies': [2, 2], 'YouTube': [3, 3]}  # Named obstacles\n",
    "        self.target_pos = [size - 1, size - 1]\n",
    "        self.cell_size = 60\n",
    "        self.q_table = np.zeros((size * size, 4))  # Q-table for learning\n",
    "\n",
    "        self.root = tk.Tk()\n",
    "        self.root.title(\"Grid Game\")\n",
    "        self.canvas = tk.Canvas(self.root, width=self.size * self.cell_size,\n",
    "                                height=self.size * self.cell_size + 30)\n",
    "        self.canvas.pack()\n",
    "        \n",
    "        self.status_text = self.canvas.create_text(\n",
    "            self.size * self.cell_size / 2, self.size * self.cell_size + 15,\n",
    "            text=\"Starting training...\", fill=\"black\"\n",
    "        )\n",
    "\n",
    "        self.draw_grid()\n",
    "        self.update_gui()\n",
    "\n",
    "    def draw_grid(self):\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                x1 = j * self.cell_size\n",
    "                y1 = i * self.cell_size\n",
    "                x2 = x1 + self.cell_size\n",
    "                y2 = y1 + self.cell_size\n",
    "                self.canvas.create_rectangle(x1, y1, x2, y2, fill=\"white\", outline=\"black\")\n",
    "\n",
    "    def update_gui(self):\n",
    "        self.canvas.delete(\"agent\")\n",
    "        self.canvas.delete(\"obstacle\")\n",
    "        self.canvas.delete(\"target\")\n",
    "\n",
    "        for name, pos in self.obstacles.items():\n",
    "            ox1, oy1, ox2, oy2 = self.get_cell_coords(pos)\n",
    "            self.canvas.create_rectangle(ox1, oy1, ox2, oy2, fill=\"red\", outline=\"black\", tags=\"obstacle\")\n",
    "            self.canvas.create_text((ox1 + ox2) / 2, (oy1 + oy2) / 2, text=name, fill=\"white\")\n",
    "\n",
    "        if self.agent_pos == self.target_pos:\n",
    "            tx1, ty1, tx2, ty2 = self.get_cell_coords(self.target_pos)\n",
    "            self.canvas.create_rectangle(tx1, ty1, tx2, ty2, fill=\"purple\", outline=\"black\", tags=\"target\")\n",
    "            self.canvas.create_text((tx1 + tx2) / 2, (ty1 + ty2) / 2, text=\"Sleep\", fill=\"white\")\n",
    "        else:\n",
    "            tx1, ty1, tx2, ty2 = self.get_cell_coords(self.target_pos)\n",
    "            self.canvas.create_rectangle(tx1, ty1, tx2, ty2, fill=\"green\", outline=\"black\", tags=\"target\")\n",
    "            self.canvas.create_text((tx1 + tx2) / 2, (ty1 + ty2) / 2, text=\"Sleep\", fill=\"white\")\n",
    "\n",
    "        ax1, ay1, ax2, ay2 = self.get_cell_coords(self.agent_pos)\n",
    "        self.canvas.create_rectangle(ax1, ay1, ax2, ay2, fill=\"blue\", outline=\"black\", tags=\"agent\")\n",
    "        self.canvas.create_text((ax1 + ax2) / 2, (ay1 + ay2) / 2, text=\"Rushi\", fill=\"white\")\n",
    "\n",
    "    def get_cell_coords(self, pos):\n",
    "        x1 = pos[1] * self.cell_size\n",
    "        y1 = pos[0] * self.cell_size\n",
    "        x2 = x1 + self.cell_size\n",
    "        y2 = y1 + self.cell_size\n",
    "        return x1, y1, x2, y2\n",
    "\n",
    "    def state_to_index(self, pos):\n",
    "        return pos[0] * self.size + pos[1]\n",
    "\n",
    "    def choose_action(self, state_index, epsilon=0.1):\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            return random.randint(0, 3)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state_index])\n",
    "\n",
    "    def learn(self, old_state, new_state, action, reward, alpha=0.1, gamma=0.9):\n",
    "        old_value = self.q_table[old_state, action]\n",
    "        future_reward = np.max(self.q_table[new_state])\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * future_reward)\n",
    "        self.q_table[old_state, action] = new_value\n",
    "\n",
    "    def move_agent(self, action):\n",
    "        new_pos = self.agent_pos.copy()\n",
    "        if action == 0 and new_pos[0] > 0:\n",
    "            new_pos[0] -= 1\n",
    "        elif action == 1 and new_pos[0] < self.size - 1:\n",
    "            new_pos[0] += 1\n",
    "        elif action == 2 and new_pos[1] > 0:\n",
    "            new_pos[1] -= 1\n",
    "        elif action == 3 and new_pos[1] < self.size - 1:\n",
    "            new_pos[1] += 1\n",
    "\n",
    "        if new_pos in [pos for name, pos in self.obstacles.items()]:\n",
    "            new_pos = [0, 0]\n",
    "\n",
    "        self.agent_pos = new_pos\n",
    "        self.update_gui()\n",
    "\n",
    "    def run_epoch(self, delay=0.1):\n",
    "        self.agent_pos = [0, 0]\n",
    "        state_index = self.state_to_index(self.agent_pos)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = self.choose_action(state_index)\n",
    "            old_state_index = state_index\n",
    "\n",
    "            self.move_agent(action)\n",
    "            state_index = self.state_to_index(self.agent_pos)\n",
    "\n",
    "            reward = self.get_reward()\n",
    "            done = self.is_done()\n",
    "            self.learn(old_state_index, state_index, action, reward)\n",
    "\n",
    "            action_text = [\"Up\", \"Down\", \"Left\", \"Right\"][action]\n",
    "            mistake_text = \"Hit obstacle\" if self.agent_pos in [pos for name, pos in self.obstacles.items()] else \"\"\n",
    "            self.canvas.itemconfig(\n",
    "                self.status_text, \n",
    "                text=f\"Action: {action_text}, Reward: {reward}, {mistake_text}\"\n",
    "            )\n",
    "\n",
    "            time.sleep(delay)\n",
    "            self.canvas.update()\n",
    "\n",
    "            if self.agent_pos == self.target_pos:\n",
    "                self.target_reached_count += 1\n",
    "                if self.target_reached_count >= self.target_threshold:\n",
    "                    self.canvas.create_text(\n",
    "                        self.size * self.cell_size / 2, self.size * self.cell_size / 2, \n",
    "                        text=\"Rushi reached Sleep!\", fill=\"purple\", font=(\"Helvetica\", 16)\n",
    "                    )\n",
    "                    return True\n",
    "            else:\n",
    "                self.target_reached_count = 0\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_reward(self):\n",
    "        if self.agent_pos == self.target_pos:\n",
    "            return 100\n",
    "        elif self.agent_pos in [pos for name, pos in self.obstacles.items()]:\n",
    "            return -100\n",
    "        return -1\n",
    "\n",
    "    def is_done(self):\n",
    "        return self.agent_pos == self.target_pos\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            if self.run_epoch(0.05):\n",
    "                print(f\"Training stopped early at epoch {epoch + 1}\")\n",
    "                break\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs} completed\")\n",
    "\n",
    "        # Save the Q-table model\n",
    "        np.save(\"q_table.npy\", self.q_table)\n",
    "        print(\"Model saved.\")\n",
    "\n",
    "    def start(self):\n",
    "        self.train()\n",
    "        self.root.mainloop()\n",
    "\n",
    "# Usage\n",
    "game = GridGameGUI()\n",
    "game.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
